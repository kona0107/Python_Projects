import numpy as np
import pandas as pd
import os
import joblib
import sys
import random
import multiprocessing
import re
import config
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from hyperopt import fmin, tpe, Trials, STATUS_OK
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import hyperopt.hp as hp 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates


def get_param_space(model_name):
    """ 
    config.pyì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì™€ hyperopt í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    
    ë§¤ê°œë³€ìˆ˜:
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: 'lgbm', 'rf', 'xgb', 'gb')
    
    ë°˜í™˜ê°’:
        dict: hyperopt ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„
    """
    param_ranges = config.HYPERPARAMS[model_name] # ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
    
    param_space = {}
    for key, value in param_ranges.items():
        if len(value) == 3:  # quniform (ì •ìˆ˜í˜•)
            param_space[key] = hp.quniform(key, *value)
        elif len(value) == 2:  # loguniform (í•™ìŠµë¥  ë“±)
            param_space[key] = hp.uniform(key, value[0], value[1])  # loguniform ëŒ€ì‹  uniform ì‚¬ìš©
    
    return param_space


def train_model(model_cls, param_space, n_iter_count, save_path, model_name, 
                X_train_scaled, X_test_scaled, y_train, y_test, train_data_sorted, 
                test_data_sorted, landscape):
    """ 
    ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
    
    ë§¤ê°œë³€ìˆ˜:
        model_cls (class): ì‚¬ìš©í•  ëª¨ë¸ í´ë˜ìŠ¤ (ì˜ˆ: LGBMRegressor, RandomForestRegressor ë“±)
        param_space (dict): hyperoptì—ì„œ ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„
        n_iter_count (int): í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë°˜ë³µ íšŸìˆ˜
        save_path (str): í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        model_name (str): ëª¨ë¸ ì´ë¦„ (ì˜ˆ: 'lgbm', 'rf', 'gb', 'xgb')
        X_train_scaled (pd.DataFrame): ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°
        X_test_scaled (pd.DataFrame): ì •ê·œí™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_train (pd.Series): í›ˆë ¨ ë°ì´í„°ì˜ íƒ€ê²Ÿ ë³€ìˆ˜
        y_test (pd.Series): í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ íƒ€ê²Ÿ ë³€ìˆ˜
        train_data_sorted (pd.DataFrame): ì •ë ¬ëœ í›ˆë ¨ ë°ì´í„°
        test_data_sorted (pd.DataFrame): ì •ë ¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        landscape (int): ê²½ê´€ ìš”ì†Œ ì„ íƒ ì˜µì…˜
    
    ë°˜í™˜ê°’:
        tuple: (í•™ìŠµëœ ìµœì  ëª¨ë¸, ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°, í›ˆë ¨ ë°ì´í„° RÂ², í…ŒìŠ¤íŠ¸ ë°ì´í„° RÂ², í›ˆë ¨ ë°ì´í„° RMSE, í…ŒìŠ¤íŠ¸ ë°ì´í„° RMSE, ëª¨ë¸ ì´ë¦„)
    """
    
    np.random.seed(42)
    random.seed(42)
    rstate = np.random.default_rng(42) # ëœë¤ ì‹œë“œ ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)

    def objective(params):
        """ 
        ìµœì í™” ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª©ì  í•¨ìˆ˜ 
        """
        for key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'num_leaves', 'min_child_samples']:
            if key in params:
                params[key] = int(params[key])

        model = model_cls(**params, random_state=42)   # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        # TimeSeriesSplit ì ìš© (3ê°œ ë¶„í•  ì‚¬ìš©)
        tscv = TimeSeriesSplit(n_splits=3)
        rmse_scores = []

        for train_idx, val_idx in tscv.split(X_train_scaled):
            X_train_fold, X_val_fold = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[val_idx]
            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

            model.fit(X_train_fold, y_train_fold)
            y_pred = model.predict(X_val_fold)
            rmse_scores.append(np.sqrt(mean_squared_error(y_val_fold, y_pred)))

        avg_rmse = np.mean(rmse_scores)  # TimeSeriesSplit ê¸°ë°˜ RMSE ê³„ì‚°
        return {'loss': avg_rmse, 'status': STATUS_OK}

    log_filename = f"{model_name}_training.log"
    log_filepath = os.path.join(save_path, log_filename)

    original_stdout = sys.stdout  # ê¸°ì¡´ í‘œì¤€ ì¶œë ¥ì„ ì €ì¥

    try:
        os.makedirs(save_path, exist_ok=True)  
        sys.stdout = open(log_filepath, 'w', encoding="utf-8")
        sys.stderr = sys.stdout  

        trials = Trials()
        best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, 
                        max_evals=n_iter_count, trials=trials, rstate=rstate)  # ğŸ”¥ hyperopt ì‹œë“œ ê³ ì •

        # ğŸ”¥ ìµœì  íŒŒë¼ë¯¸í„° ë³€í™˜ ì¶”ê°€(floatìœ¼ë¡œ ë°˜í™˜ë¼ì„œ íŒŒë¼ë¯¸í„°ê°€ 0ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ê²ƒ ë°©ì§€)
        for key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'num_leaves', 'min_child_samples']:
            if key in best_params:
                best_params[key] = int(best_params[key])

        best_model = model_cls(**best_params, random_state=42)  
        best_model.fit(X_train_scaled, y_train)  

        joblib.dump(best_model, os.path.join(save_path, f"{model_name}.pkl"))

        y_pred_train = best_model.predict(X_train_scaled)
        y_pred_test = best_model.predict(X_test_scaled)

        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

        print(f"ğŸ“Š Best Hyperparameters for {model_name}: {best_params}")
        print(f"âœ… {model_name} RMSE (Train): {train_rmse:.4f}, RMSE (Test): {test_rmse:.4f}")
        print(f"âœ… {model_name} RÂ² (Train): {train_r2:.4f}, RÂ² (Test): {test_r2:.4f}")

        print(f"ğŸ›  DEBUG: {model_name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ, load_and_predict() í˜¸ì¶œ ì‹œì‘", flush=True)

        try:
            load_and_predict(save_path, model_name, X_test_scaled, train_data_sorted, test_data_sorted, 
                            best_params, train_rmse, test_rmse, train_r2, test_r2, landscape, n_iter_count)
            print(f"ğŸ¯ {model_name} ì˜ˆì¸¡ ë° ì‹œê°í™” ì™„ë£Œ", flush=True)
        except Exception as e:
            print(f"âš ï¸ ERROR: {model_name} ì˜ˆì¸¡ ë° ì‹œê°í™”ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)

    finally:
        try:
            sys.stdout.flush()  # ë¡œê·¸ ìœ ì‹¤ ë°©ì§€
            sys.stdout.close()
        except Exception as e:
            print(f"âš ï¸ ERROR: ë¡œê·¸ íŒŒì¼ ë‹«ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        
        sys.stdout = original_stdout  # í‘œì¤€ ì¶œë ¥ ë³µì›

    return best_model, best_params, train_r2, test_r2, train_rmse, test_rmse, model_name



def run_models(n_iter_count, save_path, X_train_scaled, X_test_scaled, y_train, y_test, train_data_sorted, test_data_sorted, landscape):
    """ 
    ì—¬ëŸ¬ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í•™ìŠµì‹œí‚¤ê³  ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’:
        dict: ëª¨ë¸ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” í•™ìŠµ ê²°ê³¼ (ëª¨ë¸ ê°ì²´, í•˜ì´í¼íŒŒë¼ë¯¸í„°, í‰ê°€ ì§€í‘œ í¬í•¨)
    """
    models = {
        'lgbm': (LGBMRegressor, get_param_space('lgbm')),
        'rf': (RandomForestRegressor, get_param_space('rf')),
        'gb': (GradientBoostingRegressor, get_param_space('gb')),
        'xgb': (XGBRegressor, get_param_space('xgb'))
    }

    models_to_run = {name: (cls, space) for name, (cls, space) in models.items() if name in config.MODELS_TO_RUN}
    max_cores = multiprocessing.cpu_count()
    optimal_cores = max(max_cores - 2, 1)  # ìµœì†Œ 1ê°œ ì½”ì–´ ìœ ì§€

    pool = multiprocessing.Pool(processes=optimal_cores)
    results = {}

    def log_result(model_result):
        """ 
        ê°œë³„ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜ 
        ë§¤ê°œë³€ìˆ˜:
            model_result (tuple): í•™ìŠµëœ ëª¨ë¸ì˜ ê²°ê³¼ (íŠœí”Œ í˜•ì‹)
        """
        if isinstance(model_result, tuple) and len(model_result) == 7:
            model_name = model_result[-1]  # íŠœí”Œì˜ ë§ˆì§€ë§‰ ê°’ì´ ëª¨ë¸ ì´ë¦„
            results[model_name] = model_result
            print(f"âœ… {model_name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {model_result[:3]}...")  # ì¼ë¶€ë§Œ ì¶œë ¥í•˜ì—¬ ê°€ë…ì„± ìœ ì§€
        else:
            print(f"âš ï¸ ê²°ê³¼ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {model_result}")

    for name, (cls, space) in models_to_run.items():
        pool.apply_async(
            train_model, 
            args=(cls, space, n_iter_count, save_path, name, X_train_scaled, X_test_scaled, y_train, y_test, train_data_sorted, test_data_sorted, landscape),
            callback=log_result  # âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ì‹œ ë°”ë¡œ ê²°ê³¼ ì¶œë ¥
        )

    pool.close()
    pool.join()

    return results



def get_unique_filename(directory, base_name, extension="png"):
    """ 
    ì €ì¥ëœ ê·¸ë˜í”„ íŒŒì¼ëª…ì´ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ìë™ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•˜ëŠ” í•¨ìˆ˜
    
    ë§¤ê°œë³€ìˆ˜:
        directory (str): íŒŒì¼ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        base_name (str): íŒŒì¼ì˜ ê¸°ë³¸ ì´ë¦„ (ì˜ˆ: "model_mean_weather")
        extension (str): íŒŒì¼ í™•ì¥ì (ê¸°ë³¸ê°’: "png")
    
    ë°˜í™˜ê°’:
        str: ê³ ìœ í•œ íŒŒì¼ ê²½ë¡œ
    """
    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    existing_files = [f for f in os.listdir(directory) if f.startswith(base_name) and f.endswith(f".{extension}")]
    
    # ê¸°ì¡´ íŒŒì¼ì—ì„œ ìˆ«ì ì°¾ê¸°
    numbers = []
    for f in existing_files:
        parts = f.split("_")
        num_part = parts[-1].split(".")[0]  # í™•ì¥ì ì• ìˆ«ì ì¶”ì¶œ
        if num_part.isdigit():
            numbers.append(int(num_part))

    next_number = max(numbers) + 1 if numbers else 1  # ê¸°ì¡´ ìˆ«ì ì¤‘ ê°€ì¥ í° ê°’ +1, ì—†ìœ¼ë©´ 1

    return os.path.join(directory, f"{base_name}_{next_number}.{extension}")

def load_and_predict(save_path, model_name, X_test_scaled, train_data_sorted, test_data_sorted, 
                     best_params, train_rmse, test_rmse, train_r2, test_r2, landscape, n_iter_count):
    """ 
    ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    """
    #  (ì›”, ì¼) ë‹¨ìœ„ í‰ê·  ë°ì´í„° ê³„ì‚° 
    test_data_sorted["Month"] = test_data_sorted["DATE"].dt.month
    test_data_sorted["Day"] = test_data_sorted["DATE"].dt.day

    # ì‹¤ì œ ëª¨ê¸° ê°œì²´ ìˆ˜ í‰ê·  ê³„ì‚°
    test_data_avg = test_data_sorted.groupby(["Month", "Day"])["mosquito"].mean().reset_index()
    test_data_avg["Date"] = pd.to_datetime(test_data_avg[["Month", "Day"]].assign(Year=2024))
    test_data_avg.set_index("Date", inplace=True)
    ''' 
    ë©€í‹°í”„ë¡œì„¸ì‹±(multiprocessing) í™˜ê²½ì—ì„œ ì „ì—­ ë³€ìˆ˜ë¥¼ ê³µìœ í•˜ëŠ” ê²ƒì´ ë³µì¡í•˜ë¯€ë¡œ, ê° ëª¨ë¸ ì‹¤í–‰ ì‹œë§ˆë‹¤ test_data_avgë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ë°©ì‹
    ì¦‰ run_models()ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ëŠ” ëŒ€ì‹ , train_model()ì—ì„œ í•„ìš”í•  ë•Œë§ˆë‹¤ ìƒì„±í•˜ë„ë¡ ë³€ê²½
    '''
    # ëª¨ë¸ í’€ë„¤ì„ ë° ìƒ‰ìƒ ì •ë³´ (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì •ì˜)
    model_full_names = {
        "lgbm": "Light Gradient Boosting",
        "xgb": "Extreme Gradient Boosting",
        "rf": "Random Forest",
        "gb": "Gradient Boosting"
    }

    model_colors = {
        "lgbm": "green",
        "xgb": "red",
        "rf": "blue",
        "gb": "orange"
    }
    
    # ëª¨ë¸ë³„ ì‹¤í–‰ë§ˆë‹¤ ì´ˆê¸°í™” 
    predictions = {}  
    y_pred_avg = {}
    full_name = model_full_names.get(model_name, model_name)

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model_path = os.path.join(save_path, f"{model_name}.pkl")
    if not os.path.exists(model_path):
        print(f"âš ï¸ {model_full_names[model_name]} ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return  # âœ… ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ

    model = joblib.load(model_path)
    test_data_sorted[model_name] = model.predict(X_test_scaled)  # âœ… ëª¨ë¸ë³„ ì»¬ëŸ¼ì— ì €ì¥

    # ì˜ˆì¸¡ê°’ì„ DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì›”-ì¼ ë‹¨ìœ„ í‰ê·  ê³„ì‚°
    y_pred_df = pd.DataFrame({"DATE": test_data_sorted["DATE"], model_name: test_data_sorted[model_name]})
    y_pred_df["Month"] = y_pred_df["DATE"].dt.month
    y_pred_df["Day"] = y_pred_df["DATE"].dt.day

    # ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ ì €ì¥
    y_pred_avg[model_name] = y_pred_df.groupby(["Month", "Day"])[model_name].mean().reset_index()
    y_pred_avg[model_name].rename(columns={model_name: "Prediction"}, inplace=True)  # âœ… "Prediction" ì»¬ëŸ¼ ì¶”ê°€
    y_pred_avg[model_name]["Date"] = pd.to_datetime(y_pred_avg[model_name][["Month", "Day"]].assign(Year=2024))
    y_pred_avg[model_name].set_index("Date", inplace=True)

    print(f"ğŸ“Œ {model_name} í‰ê·  ì˜ˆì¸¡ ë°ì´í„° ìƒì„± ì™„ë£Œ")

    # ì˜ˆì¸¡ê°’ ë° í‰ê°€ ì§€í‘œ ì €ì¥ (ëª¨ë¸ë³„ ë…ë¦½ì  ìœ ì§€)
    predictions[model_name] = {
        "y_pred": test_data_sorted[model_name].values,
        "train_r2": train_r2,
        "test_r2": test_r2,
        "train_rmse": train_rmse,
        "test_rmse": test_rmse
    }
    print(f"ğŸ“Œ {model_name} ì˜ˆì¸¡ê°’ ë° í‰ê°€ ì§€í‘œ ì €ì¥ ì™„ë£Œ")

    # ê·¸ë˜í”„ ì¶œë ¥ ë° ì €ì¥
    plt.figure(figsize=(16, 8))
    # ì‹¤ì œ ëª¨ê¸° ê°œì²´ ìˆ˜ í‰ê· ê°’
    plt.plot(test_data_avg.index, test_data_avg['mosquito'], label="Observation",
             color="grey", marker='o', markersize=7, linewidth=2)
    # ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ ì‹œê°í™”
    plt.plot(y_pred_avg[model_name].index, y_pred_avg[model_name]["Prediction"], 
             label="Prediction", color=model_colors[model_name], 
             marker='o', markersize=7, linewidth=2)
    # Xì¶• í¬ë§· ì„¤ì •
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%y-%m'))
    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
    # yì¶• íšŒìƒ‰ ì„¸ë¡œì¤„ ì œê±° (xì¶•ë§Œ ê·¸ë¦¬ë“œ ìœ ì§€)
    plt.grid(axis='x')
    # yì¶• ë²”ìœ„ ìë™ ì¡°ì • (ìµœëŒ€ê°’ì´ 100 ì´í•˜ì´ë©´ 0~100ìœ¼ë¡œ ì„¤ì •, ì´ˆê³¼í•˜ë©´ ìë™ ì¡°ì •)
    max_value = max(test_data_avg['mosquito'].max(), y_pred_avg[model_name]['Prediction'].max())
    if max_value > 100:
        plt.ylim(None)  # ìë™ ì¡°ì •
    else:
        plt.ylim(0, 100)  # ê¸°ë³¸ê°’ ì„¤ì •  
    # ì œëª© ìˆ˜ì •: ì„ íƒí•œ ëª¨ë¸ì˜ í’€ë„¤ì„ì´ ë‚˜ì˜¤ë„ë¡ ë³€ê²½
    plt.title(f"{full_name}", fontsize=25)
    plt.legend(fontsize=17, loc='upper left')
    # RÂ² ë° RMSE ì •ë³´ ë°•ìŠ¤
    textstr = (f"RÂ² (Train): {train_r2:.4f}  RMSE (Train): {train_rmse:.4f}\n"
               f"RÂ² (Test): {test_r2:.4f}  RMSE (Test): {test_rmse:.4f}\n"
               f"Trial: {n_iter_count}  Landscape: {landscape}")
    plt.figtext(0.753, 0.853, textstr, fontsize=15, ha='center', va='top',
                bbox=dict(facecolor='white', edgecolor='lightgrey', boxstyle='round,pad=0.5'))
    
    #  ê·¸ë˜í”„ ì €ì¥
    base_filename = f"{model_name}_mean_ld{landscape}"
    save_path = get_unique_filename(save_path, base_filename, "png")  # ì¤‘ë³µ ë°©ì§€ ì ìš©
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")



## tasklist | findstr python

## taskkill /F /IM python3.11.exe
