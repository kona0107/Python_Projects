import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import matplotlib.dates as mdates
from glob import glob
from math import sqrt

# Scikit-learn ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK

# ì¶”ê°€ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë° ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
# from skopt import BayesSearchCV -> hyperoptë¡œ ë³€ê²½


def data_preprocessing(path):
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_excel(path, engine='openpyxl')

    # IQR ê³„ì‚°
    Q1_Mosquitoe = df['mosquito'].quantile(0.25)
    Q3_Mosquitoe = df['mosquito'].quantile(0.75)
    IQR_Mosquitoe = Q3_Mosquitoe - Q1_Mosquitoe
    lower_bound_Mosquitoe = Q1_Mosquitoe - 1.5 * IQR_Mosquitoe
    upper_bound_Mosquitoe = Q3_Mosquitoe + 1.5 * IQR_Mosquitoe

    # ì´ìƒì¹˜ ì œê±°
    df_iqr = df[(df['mosquito'] >= lower_bound_Mosquitoe) & (df['mosquito'] <= upper_bound_Mosquitoe)]

    # Train-Test ë¶„ë¦¬
    train = df_iqr[df_iqr['DATE'] <= '2023-10-31']
    test = df_iqr[df_iqr['DATE'] >= '2024-04-01']

    # ë‚ ì§œ(datetime64) ì»¬ëŸ¼ ì œê±°
    if 'DATE' in train.columns:
        train = train.drop(columns=['DATE'])
        test = test.drop(columns=['DATE'])

    # ë²”ì£¼í˜•(object) ë°ì´í„° ì œê±°
    train = train.select_dtypes(exclude=['object'])
    test = test.select_dtypes(exclude=['object'])

    # MinMaxScaler ì ìš©
    scaler = MinMaxScaler()
    train_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)
    test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns)

    # ë…ë¦½ ë³€ìˆ˜(X)ì™€ ì¢…ì† ë³€ìˆ˜(y) ë¶„ë¦¬
    target_col = 'mosquito'
    X_train = train_scaled.drop(columns=[target_col])
    y_train = train_scaled[target_col]
    X_test = test_scaled.drop(columns=[target_col])
    y_test = test_scaled[target_col]

    # ë°ì´í„° í¬ê¸° í™•ì¸
    print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

    # ë°ì´í„° ë°˜í™˜
    return X_train, y_train, X_test, y_test, train, test

# ê³µí†µ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (hyperopt ì ìš©)
def train_model(model_cls, param_space, path, n_iter_count, save_path, region, model_name):
    
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    X_train, y_train, X_test, y_test, train, test = data_preprocessing(path)

    def objective(params):
        """ ìµœì í™”í•  ëª©ì  í•¨ìˆ˜ """
        # ğŸ”¥ float -> int ë³€í™˜ (ì¤‘ìš”)
        for key in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'num_leaves', 'min_child_samples']:
            if key in params:
                params[key] = int(params[key])


        model = model_cls(**params)  # ëª¨ë¸ ì´ˆê¸°í™”
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # ëª¨ë¸ í‰ê°€ (RMSE ê³„ì‚°)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        return {'loss': rmse, 'status': STATUS_OK}

    # Hyperparameter Optimization ì‹¤í–‰
    trials = Trials()
    best_params = fmin(
        fn=objective,
        space=param_space,
        algo=tpe.suggest,  # Bayesian Optimization (TPE)
        max_evals=n_iter_count,
        trials=trials
    )

    # ğŸ”¥ ìµœì  íŒŒë¼ë¯¸í„° ë³€í™˜ ì¶”ê°€ (ë°˜ë“œì‹œ fmin() ì´í›„ì— ìœ„ì¹˜)
    for key in ['n_estimators','max_depth', 'min_samples_split', 'min_samples_leaf', 'num_leaves', 'min_child_samples']:
        if key in best_params:
            best_params[key] = int(best_params[key])

    # ìµœì  ëª¨ë¸ í•™ìŠµ
    best_model = model_cls(**best_params)
    best_model.fit(X_train, y_train)

    # ìµœì  ëª¨ë¸ ì €ì¥
    os.makedirs(save_path, exist_ok=True)
    joblib.dump(best_model, os.path.join(save_path, f"{model_name}.pkl"))

    # ì„±ëŠ¥ í‰ê°€
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    print(f"ğŸ“Š Best Hyperparameters for {model_name}: {best_params}")
    print(f"âœ… {model_name} RMSE (Train): {train_rmse:.4f}, RMSE (Test): {test_rmse:.4f}")
    print(f"âœ… {model_name} RÂ² (Train): {train_r2:.4f}, RÂ² (Test): {test_r2:.4f}")

    return best_params, best_model, train_r2, test_r2, train_rmse, test_rmse, model_name


# ëª¨ë¸ ì‹¤í–‰ í•¨ìˆ˜ (hyperopt ì ìš©)
def run_models(path, n_iter_count, save_path, region):

    models = {
        "lgbm": (LGBMRegressor, {
            'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
            'max_depth': hp.quniform('max_depth', 1, 200, 1),
            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),
            'num_leaves': hp.quniform('num_leaves', 2, 100, 1),
            'min_child_samples': hp.quniform('min_child_samples', 1, 50, 1)
        }),
        "rf": (RandomForestRegressor, {
            'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
            'max_depth': hp.quniform('max_depth', 1, 200, 1),
            'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),
            'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 20, 1)
        }),
        "gb": (GradientBoostingRegressor, {
            'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
            'max_depth': hp.quniform('max_depth', 1, 200, 1),
            'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),
            'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 20, 1),
            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0))
        }),
        "xgb": (XGBRegressor, {
            'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
            'max_depth': hp.quniform('max_depth', 1, 200, 1),
            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),
            'subsample': hp.uniform('subsample', 0.1, 1.0),
            'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0)
        })
    }

    results = {}
    for model_name, (model_cls, param_space) in models.items():
        results[model_name] = train_model(model_cls, param_space, path, n_iter_count, save_path, region, model_name)

    return results


# ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜(ì„ì‹œ)
def load_and_predict(path, save_path, model_name):
    """ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆì¸¡ ìˆ˜í–‰ í›„ ì‹œê°í™”"""
    _, _, _, _, train, test = data_preprocessing(path)
    model = joblib.load(os.path.join(save_path, f"{model_name}.pkl"))
    
    y_test_pred = model.predict(test.drop(columns=['DATE', 'mosquito']))
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(10, 5))
    plt.plot(test['DATE'], test['mosquito'], label='Actual', color='black')
    plt.plot(test['DATE'], y_test_pred, label='Predicted', color='red')
    plt.legend()
    plt.xlabel("Date")
    plt.ylabel("Mosquito Count")
    plt.title(f"{model_name} Prediction")
    plt.grid()
    plt.show()

# ì‹¤í–‰ ì˜ˆì‹œ
path = r'/home/kona0107/EDAM/2015_2024_total.xlsx'
save_path = r'/home/kona0107/EDAM/models'
n_iter_count = 1
region = "Seoul"

results = run_models(path, n_iter_count, save_path, region)

